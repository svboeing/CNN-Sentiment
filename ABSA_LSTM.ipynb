{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "tree = ET.parse('ABSA-15_Restaurants_Train_Final.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "words = []\n",
    "vecs = []\n",
    "with open(\"fasttext150\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if skip:\n",
    "            info = line.strip().split(' ')\n",
    "            words.append(info[0]) \n",
    "            c = []\n",
    "            for i in range(1, len(info)):\n",
    "                c.append(float(info[i]))\n",
    "            vecs.append(c)\n",
    "        else:\n",
    "            skip = True\n",
    "npvecs = np.asarray(vecs, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = {}\n",
    "\n",
    "vecs_lookup = np.concatenate([np.zeros((1, 100), dtype='float32'), npvecs], axis=0)\n",
    "_id = 1\n",
    "#counter = 0\n",
    "for word in words:\n",
    "    if word not in word_id:\n",
    "        word_id[word] = _id\n",
    "        #vecs_lookup.append(vecs[counter])\n",
    "        #word_id[word] = [_id, vecs[counter]]\n",
    "        _id += 1\n",
    "#    counter += 1\n",
    "#word_id[\"%NOWORD%\"] = [0, np.zeros(100)] #NOWORD is for no word. id = 0. embedding = 0 0 0 0 0 0..\n",
    "#vecs_lookup = np.insert(vecs_lookup, obj= 0, values = np.zeros(100), axis = 0) #id = 0 is reserved for embedding = 0 0 0 0 0 0 0 0...\n",
    "\n",
    "#print(vecs_lookup[word_id['zero']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "opins = []\n",
    "for review in root:\n",
    "    for sentence in review[0]:\n",
    "        #sentence[0] #text\n",
    "        if len(sentence) > 1:\n",
    "            _ = re.sub(r\"[\\.\\-\\?\\!,\\\"\\']\", \" \", sentence[0].text.lower()).strip().split(' ')\n",
    "            __ = []\n",
    "            for word in _:\n",
    "                if word != \"\":#get rid of \"\"s\n",
    "                    #__.append(word)\n",
    "                    if word in word_id:\n",
    "                        __.append(word_id[word])\n",
    "                    else:\n",
    "                        __.append(0)\n",
    "            if len(__) != 0:\n",
    "                sents.append(__)\n",
    "                opinions = [opin.attrib for opin in sentence[1]]\n",
    "                opins.append(opinions)\n",
    "            #data[sentence[0].text] = [opin.attrib for opin in sentence[1]]\n",
    "        #print(sentence[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(shape(sents))\n",
    "#print(sents[120:140], opins[120:140])\n",
    "#print(sents[:14], opins[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(zip(sents,opins))\n",
    "_ = sorted(zip(sents,opins), key=lambda pair: len(pair[0]))\n",
    "sents, opins = zip(*_)\n",
    "\n",
    "#print(sents[120:140], opins[120:140]) #seems okay\n",
    "#print(sents[:20], opins[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'to': '10', 'target': 'waiter', 'from': '4', 'category': 'SERVICE#GENERAL', 'polarity': 'positive'}], [{'to': '18', 'target': 'place', 'from': '13', 'category': 'RESTAURANT#GENERAL', 'polarity': 'positive'}], [{'to': '0', 'target': 'NULL', 'from': '0', 'category': 'RESTAURANT#MISCELLANEOUS', 'polarity': 'positive'}], [{'to': '26', 'target': 'service', 'from': '19', 'category': 'SERVICE#GENERAL', 'polarity': 'positive'}], [{'to': '0', 'target': 'NULL', 'from': '0', 'category': 'SERVICE#GENERAL', 'polarity': 'positive'}]) [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#print(sents[:14], opins[:14])\n",
    "#print(opins[14][0]['polarity'])\n",
    "classes_labels = {'positive':1, 'negative':-1, 'neutral':0}\n",
    "dumb_Y = []\n",
    "for opin  in opins:\n",
    "    _ = [target_op['polarity'] for target_op in opin]\n",
    "    __ = np.asarray([classes_labels[class_] for class_ in _], dtype = 'int32')\n",
    "    dumb_Y.append(round(np.mean(__, dtype = 'int32'))) #labels are crushed into a mean\n",
    "        \n",
    "        \n",
    "print(opins[120:125], dumb_Y[120:125])\n",
    "#print(word_id['amazing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_maker(sents, dumb_Y, min_samples, sample_size, pos):\n",
    "    #sample size not fixed!!!!!!!!!!!!!!\n",
    "    batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    while len(batch) < min_samples and pos < len(dumb_Y): #not really\n",
    "        z = [0,0,0]\n",
    "        #print(\"i got here pos = \", pos)\n",
    "        #print(len(dumb_Y))\n",
    "        index_to_set_1 = dumb_Y[pos]\n",
    "        z[1+index_to_set_1] = 1 #one-hot dumb_Y prepared for ith sent\n",
    "        \n",
    "        if len(sents[pos]) <= sample_size:\n",
    "            _ = sents[pos][0:]\n",
    "            #while len(_) !=  sample_size:\n",
    "            #    _.append(0)\n",
    "            if len(_) != sample_size:\n",
    "                _ = np.concatenate((_, np.zeros(sample_size - len(_), dtype = \"int32\")))\n",
    "            y_batch.append(z)\n",
    "            batch.append(np.asarray(_, dtype = \"int32\"))\n",
    "            \n",
    "            pos += 1 #sentence processed\n",
    "        else:\n",
    "            counter = 0\n",
    "            while counter*sample_size < len(sents[pos]):\n",
    "                _ = sents[pos][counter*sample_size:(counter+1)*sample_size]\n",
    "                #while len(_) !=  sample_size:\n",
    "                #    _.append(0)\n",
    "                if len(_) != sample_size:\n",
    "                    _ = np.concatenate((_, np.zeros(sample_size - len(_), dtype = \"int32\")))\n",
    "                counter += 1\n",
    "                y_batch.append(z)\n",
    "                batch.append(np.asarray(_, dtype = \"int32\"))\n",
    "                \n",
    "            pos += 1 #sentence processed\n",
    "    batch = np.asarray(batch)\n",
    "    y_batch = np.asarray(y_batch)\n",
    "    return batch, y_batch, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 6348,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [ 7705,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [  642, 39524,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]], dtype=int32), array([[0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1]]), 4)\n",
      "([6348], [0], [7705], [642, 39524], [856, 4258], [3027, 1028], [0, 28883]) [1, 1, 1, 1, 1, 1, 1]\n",
      "+ 762 0 114 - 244\n"
     ]
    }
   ],
   "source": [
    "print(batch_maker(sents, dumb_Y, 4, 15, 0))\n",
    "print(sents[0:7], dumb_Y[0:7])\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "for a in dumb_Y:\n",
    "    if a == 1:\n",
    "        i += 1\n",
    "    if a == -1:\n",
    "        k += 1\n",
    "    if a == 0:\n",
    "        j += 1\n",
    "print('+', i, '0', j, '-', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 896\n"
     ]
    }
   ],
   "source": [
    "train_amount = round(len(dumb_Y)*0.8)\n",
    "train_sents = sents[:train_amount]\n",
    "train_Y = dumb_Y[:train_amount]\n",
    "test_sents = sents[train_amount:]\n",
    "test_Y = dumb_Y[train_amount:]\n",
    "print(len(train_Y), len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM with return_sequences=True returns the hidden state of the LSTM for every timestep in the input to the LSTM. For example, if the input batch is (samples, timesteps, dims), then the call LSTM(units, return_sequences=True) will generate output of dimensions (samples, timesteps, units). https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10\n",
    "#batch_size = 128 NOSUCH\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "num_input =  100\n",
    "timesteps = 50 # timesteps ~ words?\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 3\n",
    "# tf Graph input\n",
    "#X = tf.placeholder(\"float32\", [None, timesteps, num_input])\n",
    "X = tf.placeholder(\"int32\", [None, timesteps])\n",
    "Y = tf.placeholder(\"int32\", [None, num_classes]) #one hot repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = tf.constant(vecs_lookup, dtype=tf.float32) #words do have lookup too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes])) #this is tf.float32\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "    x = tf.nn.embedding_lookup(word_embs, x) #OKAY\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32) #x, dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-46c49243df5f>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-15-5705d3a38005>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = BiRNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "answers = tf.argmax(prediction, 1)\n",
    "true_labels = tf.argmax(Y, 1)\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.0277, Training Accuracy= 0.188\n",
      "Step 1, Minibatch Loss= 0.9973, Training Accuracy= 0.125\n",
      "Step 1, Minibatch Loss= 1.0228, Training Accuracy= 0.188\n",
      "Step 1, Minibatch Loss= 0.8579, Training Accuracy= 0.094\n",
      "Step 1, Minibatch Loss= 0.9059, Training Accuracy= 0.125\n",
      "Step 1, Minibatch Loss= 0.8389, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.7802, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.7929, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.7767, Training Accuracy= 0.781\n",
      "Step 1, Minibatch Loss= 0.7214, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.8263, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.9736, Training Accuracy= 0.531\n",
      "Step 1, Minibatch Loss= 0.7590, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.7765, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.9541, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.9723, Training Accuracy= 0.688\n",
      "Step 1, Minibatch Loss= 0.8401, Training Accuracy= 0.688\n",
      "Step 1, Minibatch Loss= 0.8577, Training Accuracy= 0.719\n",
      "Step 1, Minibatch Loss= 0.7909, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 1.1339, Training Accuracy= 0.531\n",
      "Step 1, Minibatch Loss= 0.7291, Training Accuracy= 0.719\n",
      "Step 1, Minibatch Loss= 0.8361, Training Accuracy= 0.562\n",
      "Step 1, Minibatch Loss= 0.7040, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.7392, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 1.0429, Training Accuracy= 0.500\n",
      "Step 1, Minibatch Loss= 1.0851, Training Accuracy= 0.594\n",
      "Step 1, Minibatch Loss= 0.6994, Training Accuracy= 0.844\n",
      "Step 1, Minibatch Loss= 0.7322, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 2, Minibatch Loss= 0.6625, Training Accuracy= 0.781\n",
      "Step 2, Minibatch Loss= 0.5987, Training Accuracy= 0.844\n",
      "Step 2, Minibatch Loss= 0.7872, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.4862, Training Accuracy= 0.906\n",
      "Step 2, Minibatch Loss= 0.6488, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.7735, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.7342, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.6100, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.6404, Training Accuracy= 0.781\n",
      "Step 2, Minibatch Loss= 0.6342, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.6777, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 1.0210, Training Accuracy= 0.531\n",
      "Step 2, Minibatch Loss= 0.6405, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.7085, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.9232, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.9201, Training Accuracy= 0.688\n",
      "Step 2, Minibatch Loss= 0.8143, Training Accuracy= 0.688\n",
      "Step 2, Minibatch Loss= 0.8129, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.7432, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 1.1514, Training Accuracy= 0.531\n",
      "Step 2, Minibatch Loss= 0.7123, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.8803, Training Accuracy= 0.562\n",
      "Step 2, Minibatch Loss= 0.6769, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.7098, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 1.0801, Training Accuracy= 0.500\n",
      "Step 2, Minibatch Loss= 1.0740, Training Accuracy= 0.594\n",
      "Step 2, Minibatch Loss= 0.6421, Training Accuracy= 0.844\n",
      "Step 2, Minibatch Loss= 0.7110, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 3, Minibatch Loss= 0.6410, Training Accuracy= 0.781\n",
      "Step 3, Minibatch Loss= 0.5645, Training Accuracy= 0.844\n",
      "Step 3, Minibatch Loss= 0.7767, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.4483, Training Accuracy= 0.906\n",
      "Step 3, Minibatch Loss= 0.6263, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.7885, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.7519, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.5945, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.6319, Training Accuracy= 0.781\n",
      "Step 3, Minibatch Loss= 0.6354, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.6588, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 1.0416, Training Accuracy= 0.531\n",
      "Step 3, Minibatch Loss= 0.6263, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.7040, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.9214, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.9111, Training Accuracy= 0.688\n",
      "Step 3, Minibatch Loss= 0.8148, Training Accuracy= 0.688\n",
      "Step 3, Minibatch Loss= 0.8069, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.7372, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 1.1534, Training Accuracy= 0.531\n",
      "Step 3, Minibatch Loss= 0.7151, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.8971, Training Accuracy= 0.562\n",
      "Step 3, Minibatch Loss= 0.6762, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.7067, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 1.0894, Training Accuracy= 0.500\n",
      "Step 3, Minibatch Loss= 1.0678, Training Accuracy= 0.594\n",
      "Step 3, Minibatch Loss= 0.6302, Training Accuracy= 0.844\n",
      "Step 3, Minibatch Loss= 0.7088, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 4, Minibatch Loss= 0.6394, Training Accuracy= 0.781\n",
      "Step 4, Minibatch Loss= 0.5593, Training Accuracy= 0.844\n",
      "Step 4, Minibatch Loss= 0.7751, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.4426, Training Accuracy= 0.906\n",
      "Step 4, Minibatch Loss= 0.6222, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.7949, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.7598, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.5926, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.6317, Training Accuracy= 0.781\n",
      "Step 4, Minibatch Loss= 0.6386, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.6538, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 1.0464, Training Accuracy= 0.531\n",
      "Step 4, Minibatch Loss= 0.6233, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.7036, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.9193, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.9063, Training Accuracy= 0.688\n",
      "Step 4, Minibatch Loss= 0.8150, Training Accuracy= 0.688\n",
      "Step 4, Minibatch Loss= 0.8045, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.7354, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 1.1504, Training Accuracy= 0.531\n",
      "Step 4, Minibatch Loss= 0.7173, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.9033, Training Accuracy= 0.562\n",
      "Step 4, Minibatch Loss= 0.6773, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.7064, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 1.0909, Training Accuracy= 0.500\n",
      "Step 4, Minibatch Loss= 1.0629, Training Accuracy= 0.594\n",
      "Step 4, Minibatch Loss= 0.6266, Training Accuracy= 0.844\n",
      "Step 4, Minibatch Loss= 0.7087, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 5, Minibatch Loss= 0.6399, Training Accuracy= 0.781\n",
      "Step 5, Minibatch Loss= 0.5586, Training Accuracy= 0.844\n",
      "Step 5, Minibatch Loss= 0.7745, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.4422, Training Accuracy= 0.906\n",
      "Step 5, Minibatch Loss= 0.6212, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.7977, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.7636, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.5927, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.6323, Training Accuracy= 0.781\n",
      "Step 5, Minibatch Loss= 0.6407, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.6519, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 1.0474, Training Accuracy= 0.531\n",
      "Step 5, Minibatch Loss= 0.6225, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.7036, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.9176, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.9034, Training Accuracy= 0.688\n",
      "Step 5, Minibatch Loss= 0.8149, Training Accuracy= 0.688\n",
      "Step 5, Minibatch Loss= 0.8031, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.7347, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 1.1477, Training Accuracy= 0.531\n",
      "Step 5, Minibatch Loss= 0.7186, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.9061, Training Accuracy= 0.562\n",
      "Step 5, Minibatch Loss= 0.6782, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.7065, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 1.0909, Training Accuracy= 0.500\n",
      "Step 5, Minibatch Loss= 1.0598, Training Accuracy= 0.594\n",
      "Step 5, Minibatch Loss= 0.6252, Training Accuracy= 0.844\n",
      "Step 5, Minibatch Loss= 0.7089, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 6, Minibatch Loss= 0.6405, Training Accuracy= 0.781\n",
      "Step 6, Minibatch Loss= 0.5588, Training Accuracy= 0.844\n",
      "Step 6, Minibatch Loss= 0.7742, Training Accuracy= 0.719\n",
      "Step 6, Minibatch Loss= 0.4425, Training Accuracy= 0.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6, Minibatch Loss= 0.6209, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.7992, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.7656, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.5930, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.6328, Training Accuracy= 0.781\n",
      "Step 6, Minibatch Loss= 0.6419, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.6510, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 1.0476, Training Accuracy= 0.531\n",
      "Step 6, Minibatch Loss= 0.6222, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.7037, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.9166, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.9017, Training Accuracy= 0.688\n",
      "Step 6, Minibatch Loss= 0.8148, Training Accuracy= 0.688\n",
      "Step 6, Minibatch Loss= 0.8024, Training Accuracy= 0.719\n",
      "Step 6, Minibatch Loss= 0.7343, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 1.1460, Training Accuracy= 0.531\n",
      "Step 6, Minibatch Loss= 0.7194, Training Accuracy= 0.719\n",
      "Step 6, Minibatch Loss= 0.9076, Training Accuracy= 0.562\n",
      "Step 6, Minibatch Loss= 0.6788, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.7066, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 1.0907, Training Accuracy= 0.500\n",
      "Step 6, Minibatch Loss= 1.0580, Training Accuracy= 0.594\n",
      "Step 6, Minibatch Loss= 0.6246, Training Accuracy= 0.844\n",
      "Step 6, Minibatch Loss= 0.7090, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 7, Minibatch Loss= 0.6410, Training Accuracy= 0.781\n",
      "Step 7, Minibatch Loss= 0.5589, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.7741, Training Accuracy= 0.719\n",
      "Step 7, Minibatch Loss= 0.4429, Training Accuracy= 0.906\n",
      "Step 7, Minibatch Loss= 0.6208, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.7999, Training Accuracy= 0.656\n",
      "Step 7, Minibatch Loss= 0.7668, Training Accuracy= 0.656\n",
      "Step 7, Minibatch Loss= 0.5932, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.6331, Training Accuracy= 0.781\n",
      "Step 7, Minibatch Loss= 0.6427, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 0.6505, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 1.0477, Training Accuracy= 0.531\n",
      "Step 7, Minibatch Loss= 0.6221, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.7038, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 0.9160, Training Accuracy= 0.656\n",
      "Step 7, Minibatch Loss= 0.9007, Training Accuracy= 0.688\n",
      "Step 7, Minibatch Loss= 0.8148, Training Accuracy= 0.688\n",
      "Step 7, Minibatch Loss= 0.8020, Training Accuracy= 0.719\n",
      "Step 7, Minibatch Loss= 0.7342, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 1.1449, Training Accuracy= 0.531\n",
      "Step 7, Minibatch Loss= 0.7199, Training Accuracy= 0.719\n",
      "Step 7, Minibatch Loss= 0.9084, Training Accuracy= 0.562\n",
      "Step 7, Minibatch Loss= 0.6791, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 0.7067, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 1.0905, Training Accuracy= 0.500\n",
      "Step 7, Minibatch Loss= 1.0569, Training Accuracy= 0.594\n",
      "Step 7, Minibatch Loss= 0.6243, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.7092, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 8, Minibatch Loss= 0.6412, Training Accuracy= 0.781\n",
      "Step 8, Minibatch Loss= 0.5591, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.7740, Training Accuracy= 0.719\n",
      "Step 8, Minibatch Loss= 0.4431, Training Accuracy= 0.906\n",
      "Step 8, Minibatch Loss= 0.6208, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.8004, Training Accuracy= 0.656\n",
      "Step 8, Minibatch Loss= 0.7674, Training Accuracy= 0.656\n",
      "Step 8, Minibatch Loss= 0.5934, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.6333, Training Accuracy= 0.781\n",
      "Step 8, Minibatch Loss= 0.6431, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 0.6503, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 1.0477, Training Accuracy= 0.531\n",
      "Step 8, Minibatch Loss= 0.6221, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.7038, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 0.9156, Training Accuracy= 0.656\n",
      "Step 8, Minibatch Loss= 0.9001, Training Accuracy= 0.688\n",
      "Step 8, Minibatch Loss= 0.8147, Training Accuracy= 0.688\n",
      "Step 8, Minibatch Loss= 0.8017, Training Accuracy= 0.719\n",
      "Step 8, Minibatch Loss= 0.7341, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 1.1443, Training Accuracy= 0.531\n",
      "Step 8, Minibatch Loss= 0.7201, Training Accuracy= 0.719\n",
      "Step 8, Minibatch Loss= 0.9089, Training Accuracy= 0.562\n",
      "Step 8, Minibatch Loss= 0.6793, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 0.7068, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 1.0904, Training Accuracy= 0.500\n",
      "Step 8, Minibatch Loss= 1.0563, Training Accuracy= 0.594\n",
      "Step 8, Minibatch Loss= 0.6241, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.7092, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 9, Minibatch Loss= 0.6414, Training Accuracy= 0.781\n",
      "Step 9, Minibatch Loss= 0.5591, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.7740, Training Accuracy= 0.719\n",
      "Step 9, Minibatch Loss= 0.4432, Training Accuracy= 0.906\n",
      "Step 9, Minibatch Loss= 0.6207, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.8007, Training Accuracy= 0.656\n",
      "Step 9, Minibatch Loss= 0.7678, Training Accuracy= 0.656\n",
      "Step 9, Minibatch Loss= 0.5935, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.6334, Training Accuracy= 0.781\n",
      "Step 9, Minibatch Loss= 0.6434, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 0.6501, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 1.0477, Training Accuracy= 0.531\n",
      "Step 9, Minibatch Loss= 0.6220, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.7038, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 0.9154, Training Accuracy= 0.656\n",
      "Step 9, Minibatch Loss= 0.8998, Training Accuracy= 0.688\n",
      "Step 9, Minibatch Loss= 0.8147, Training Accuracy= 0.688\n",
      "Step 9, Minibatch Loss= 0.8016, Training Accuracy= 0.719\n",
      "Step 9, Minibatch Loss= 0.7340, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 1.1439, Training Accuracy= 0.531\n",
      "Step 9, Minibatch Loss= 0.7203, Training Accuracy= 0.719\n",
      "Step 9, Minibatch Loss= 0.9091, Training Accuracy= 0.562\n",
      "Step 9, Minibatch Loss= 0.6794, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 0.7068, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 1.0903, Training Accuracy= 0.500\n",
      "Step 9, Minibatch Loss= 1.0559, Training Accuracy= 0.594\n",
      "Step 9, Minibatch Loss= 0.6240, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.7093, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 10, Minibatch Loss= 0.6414, Training Accuracy= 0.781\n",
      "Step 10, Minibatch Loss= 0.5592, Training Accuracy= 0.844\n",
      "Step 10, Minibatch Loss= 0.7739, Training Accuracy= 0.719\n",
      "Step 10, Minibatch Loss= 0.4433, Training Accuracy= 0.906\n",
      "Step 10, Minibatch Loss= 0.6207, Training Accuracy= 0.812\n",
      "Step 10, Minibatch Loss= 0.8008, Training Accuracy= 0.656\n",
      "Step 10, Minibatch Loss= 0.7680, Training Accuracy= 0.656\n",
      "Step 10, Minibatch Loss= 0.5935, Training Accuracy= 0.812\n",
      "Step 10, Minibatch Loss= 0.6335, Training Accuracy= 0.781\n",
      "Step 10, Minibatch Loss= 0.6435, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 0.6500, Training Accuracy= 0.812\n",
      "Step 10, Minibatch Loss= 1.0477, Training Accuracy= 0.531\n",
      "Step 10, Minibatch Loss= 0.6220, Training Accuracy= 0.812\n",
      "Step 10, Minibatch Loss= 0.7038, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 0.9153, Training Accuracy= 0.656\n",
      "Step 10, Minibatch Loss= 0.8996, Training Accuracy= 0.688\n",
      "Step 10, Minibatch Loss= 0.8147, Training Accuracy= 0.688\n",
      "Step 10, Minibatch Loss= 0.8015, Training Accuracy= 0.719\n",
      "Step 10, Minibatch Loss= 0.7340, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 1.1437, Training Accuracy= 0.531\n",
      "Step 10, Minibatch Loss= 0.7204, Training Accuracy= 0.719\n",
      "Step 10, Minibatch Loss= 0.9093, Training Accuracy= 0.562\n",
      "Step 10, Minibatch Loss= 0.6795, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 0.7068, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 1.0903, Training Accuracy= 0.500\n",
      "Step 10, Minibatch Loss= 1.0557, Training Accuracy= 0.594\n",
      "Step 10, Minibatch Loss= 0.6239, Training Accuracy= 0.844\n",
      "Step 10, Minibatch Loss= 0.7093, Training Accuracy= 0.750\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#print(len(train_Y))\n",
    "for step in range(1, training_steps+1):\n",
    "    i = 0\n",
    "    while i < len(train_sents):\n",
    "        x_batch, y_batch, i = batch_maker(train_sents, train_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "        #print(i)\n",
    "        #x_lookup = tf.nn.embedding_lookup(word_embs, x_batch)\n",
    "        #sess.run(x_lookup)\n",
    "        sess.run(train_op, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5288888888888889\n"
     ]
    }
   ],
   "source": [
    "preds = np.array([])\n",
    "i = 0\n",
    "while i < len(test_sents):\n",
    "    x_batch, y_batch, i = batch_maker(test_sents, test_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "        #print(i, x_batch.shape, y_batch.shape)\n",
    "        #x_lookup = tf.nn.embedding_lookup(word_embs, x_batch)\n",
    "        #sess.run(x_lookup)\n",
    "        #sess.run(train_op, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "        #print('HUY')\n",
    "             \n",
    "    pred = sess.run(correct_pred, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "    preds = np.append(preds, np.asarray(pred))\n",
    "print(np.mean(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0.691860465116279} 0.23062015503875966\n",
      "{0: 0, 1: 0, 2: 119} {0: 0, 1: 0, 2: 106} {0: 106, 1: 106, 2: 0} {0: 0, 1: 0, 2: 0.5288888888888889} {0: 0.0, 1: 0.0, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "all_answers = np.array([])\n",
    "all_true_labels = np.array([])\n",
    "i = 0\n",
    "\n",
    "while i < len(test_sents):\n",
    "    x_batch, y_batch, i = batch_maker(test_sents, test_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "    \n",
    "    a, t = sess.run([answers, true_labels], feed_dict={X: x_batch, Y: y_batch})\n",
    "    #print(a,t)\n",
    "    all_answers = np.append(all_answers, np.asarray(a)) #no need to flatten\n",
    "    all_true_labels = np.append(all_true_labels, np.asarray(t))\n",
    "    \n",
    "    \n",
    "TP, FP, FN = {0:0, 1:0, 2:0}, {0:0, 1:0, 2:0}, {0:0, 1:0, 2:0}\n",
    "for l in range(3):\n",
    "    for counter in range(len(all_answers)):\n",
    "        if all_answers[counter] == all_true_labels[counter] and all_answers[counter] == l: #answer matches label\n",
    "            TP[l] += 1\n",
    "        if all_answers[counter] != all_true_labels[counter] and all_answers[counter] == l:\n",
    "            FP[l] += 1\n",
    "        if all_answers[counter] != all_true_labels[counter] and all_answers[counter] != l:\n",
    "            FN[l] += 1\n",
    "            \n",
    "Prec, Rec = {}, {}\n",
    "for l in range(3):\n",
    "    try:\n",
    "        Prec[l] = TP[l]/float((TP[l]+FP[l]))\n",
    "    except Exception:\n",
    "        Prec[l] = 0\n",
    "    try:\n",
    "        Rec[l] = TP[l]/float((TP[l]+FN[l]))\n",
    "    except Exception:\n",
    "        Rec[l] = 0\n",
    "F1 = {}\n",
    "for l in range(3):\n",
    "    try:\n",
    "        F1[l] = 2*Prec[l]*Rec[l]/(Prec[l]+Rec[l])\n",
    "    except Exception:\n",
    "        F1[l] = 0\n",
    "    \n",
    "    \n",
    "MF1 = sum(_ for _ in F1.values())/3\n",
    "print(F1, MF1)\n",
    "print(TP, FP, FN, Prec, Rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
