{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "tree = ET.parse('ABSA-15_Restaurants_Train_Final.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "words = []\n",
    "vecs = []\n",
    "with open(\"fasttext150\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if skip:\n",
    "            info = line.strip().split(' ')\n",
    "            words.append(info[0]) \n",
    "            c = []\n",
    "            for i in range(1, len(info)):\n",
    "                c.append(float(info[i]))\n",
    "            vecs.append(c)\n",
    "        else:\n",
    "            skip = True\n",
    "npvecs = np.asarray(vecs, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = {}\n",
    "\n",
    "vecs_lookup = np.concatenate([np.zeros((1, 100), dtype='float32'), npvecs], axis=0)\n",
    "_id = 1\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_id:\n",
    "        word_id[word] = _id\n",
    "        \n",
    "        _id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "opins = []\n",
    "for review in root:\n",
    "    for sentence in review[0]:\n",
    "        #sentence[0] #text\n",
    "        if len(sentence) > 1:\n",
    "            _ = re.sub(r\"[\\.\\-\\?\\!,\\\"\\']\", \" \", sentence[0].text.lower()).strip().split(' ')\n",
    "            __ = []\n",
    "            for word in _:\n",
    "                if word != \"\":#get rid of \"\"s\n",
    "                    #__.append(word)\n",
    "                    if word in word_id:\n",
    "                        __.append(word_id[word])\n",
    "                    else:\n",
    "                        __.append(0)\n",
    "            if len(__) != 0:\n",
    "                sents.append(__)\n",
    "                opinions = [opin.attrib for opin in sentence[1]]\n",
    "                opins.append(opinions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sorted(zip(sents,opins), key=lambda pair: len(pair[0]))\n",
    "sents, opins = zip(*_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_labels = {'positive':1, 'negative':-1, 'neutral':0}\n",
    "dumb_Y = []\n",
    "for opin  in opins:\n",
    "    _ = [target_op['polarity'] for target_op in opin]\n",
    "    __ = np.asarray([classes_labels[class_] for class_ in _], dtype = 'int32')\n",
    "    dumb_Y.append(round(np.mean(__, dtype = 'int32'))) #labels are crushed into a mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_maker(sents, dumb_Y, min_samples, sample_size, pos):\n",
    "    #sample size not fixed!!!!!!!!!!!!!!\n",
    "    batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    while len(batch) < min_samples and pos < len(dumb_Y): #not really\n",
    "        z = [0,0,0]\n",
    "        #print(\"i got here pos = \", pos)\n",
    "        #print(len(dumb_Y))\n",
    "        index_to_set_1 = dumb_Y[pos]\n",
    "        z[1+index_to_set_1] = 1 #one-hot dumb_Y prepared for ith sent\n",
    "        \n",
    "        if len(sents[pos]) <= sample_size:\n",
    "            _ = sents[pos][0:]\n",
    "            #while len(_) !=  sample_size:\n",
    "            #    _.append(0)\n",
    "            if len(_) != sample_size:\n",
    "                _ = np.concatenate((_, np.zeros(sample_size - len(_), dtype = \"int32\")))\n",
    "            y_batch.append(z)\n",
    "            batch.append(np.asarray(_, dtype = \"int32\"))\n",
    "            \n",
    "            pos += 1 #sentence processed\n",
    "        else:\n",
    "            counter = 0\n",
    "            while counter*sample_size < len(sents[pos]):\n",
    "                _ = sents[pos][counter*sample_size:(counter+1)*sample_size]\n",
    "                #while len(_) !=  sample_size:\n",
    "                #    _.append(0)\n",
    "                if len(_) != sample_size:\n",
    "                    _ = np.concatenate((_, np.zeros(sample_size - len(_), dtype = \"int32\")))\n",
    "                counter += 1\n",
    "                y_batch.append(z)\n",
    "                batch.append(np.asarray(_, dtype = \"int32\"))\n",
    "                \n",
    "            pos += 1 #sentence processed\n",
    "    batch = np.asarray(batch)\n",
    "    y_batch = np.asarray(y_batch)\n",
    "    return batch, y_batch, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 896\n"
     ]
    }
   ],
   "source": [
    "train_amount = round(len(dumb_Y)*0.8)\n",
    "train_sents = sents[:train_amount]\n",
    "train_Y = dumb_Y[:train_amount]\n",
    "test_sents = sents[train_amount:]\n",
    "test_Y = dumb_Y[train_amount:]\n",
    "print(len(train_Y), len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a,b,c = batch_maker(sents, dumb_Y, 10, 15, 850)\n",
    "print(a,b,c)\n",
    "print(sents[850:855], dumb_Y[850:855])\n",
    "print(len(sents[1119]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "num_input =  100\n",
    "timesteps = 40 # timesteps ~ words\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 3\n",
    "\n",
    "X = tf.placeholder(\"int32\", [None, timesteps])\n",
    "Y = tf.placeholder(\"int32\", [None, num_classes]) #one hot repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = tf.constant(vecs_lookup, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 20\n",
    "width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "filters = tf.Variable(xavier_init([width, num_input, n_filters])) #Xavier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.nn.embedding_lookup(word_embs, x) #embedding - returns batch x len x emb ????\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.dropout(inputs = x, rate = 0.2)\n",
    "\n",
    "    x = tf.nn.conv1d(value=x, filters = filters, stride = 1,padding = 'VALID')\n",
    "    x = tf.nn.relu(features = x)\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.max_pooling1d(inputs = x, pool_size = timesteps - 2, strides = 1, padding = 'VALID') #pool_size = tf.shape(x)[1]\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.squeeze(x, [1]) #because of one fictional dim from global max pooling\n",
    "    x = tf.layers.dense(inputs = x, units = 250, activation = 'relu') #has his variables inside?\n",
    "    x = tf.layers.dropout(inputs = x, rate = 0.2)\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.dense(inputs = x, units = 3) #has his variables inside? softmax goes there later\n",
    "    #print(tf.shape(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-4945f570ab14>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = neural_net(X)\n",
    "\n",
    "#this WAS in LSTM. needed?\n",
    "prediction = tf.nn.softmax(logits) #so, another softmax on the output\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y)) #there is softmax applied to the last output\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "output_labels = tf.argmax(logits, 1)\n",
    "\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#print('\\n',tf.shape(prediction), tf.shape(tf.argmax(prediction, 1)), tf.shape(tf.argmax(Y, 1)))\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))#tf.argmax(logits, 1),\n",
    "\n",
    "\n",
    "answers = tf.argmax(prediction, 1)\n",
    "true_labels = tf.argmax(Y, 1)\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP(-1) = количество верно обнаруженных negative\n",
    "FP(-1) = количество неверно обнаруженных negative (система говорит negative)\n",
    "FN(-1) = количество неверно не обнаруженных negative (система говорит не negative)\n",
    "для 0 и +1 аналогично\n",
    "\n",
    "Precision(-1) = TP(-1) / (TP(-1) + FP(-1))\n",
    "Recall(-1) = TP(-1) / (TP(-1) + FN(-1))\n",
    "F-1(-1) = 2 * Precision(-1) * Recall(-1) / (Precision(-1) + Recall(-1))\n",
    "\n",
    "Macro F-1 = (F-1(-1) + F-1(0) + F-1(+1)) / 3\n",
    "Micro F-1 = P(-1)*F-1(-1) + P(0)*F-1(0) + P(+1)*F-1(+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.0638, Training Accuracy= 0.688\n",
      "Step 1, Minibatch Loss= 0.9886, Training Accuracy= 0.844\n",
      "Step 1, Minibatch Loss= 0.9595, Training Accuracy= 0.719\n",
      "Step 1, Minibatch Loss= 0.8428, Training Accuracy= 0.906\n",
      "Step 1, Minibatch Loss= 0.8188, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.9286, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.8650, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.6720, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.6879, Training Accuracy= 0.781\n",
      "Step 1, Minibatch Loss= 0.7457, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.6233, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 1.1141, Training Accuracy= 0.531\n",
      "Step 1, Minibatch Loss= 0.5828, Training Accuracy= 0.812\n",
      "Step 1, Minibatch Loss= 0.7020, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.9362, Training Accuracy= 0.656\n",
      "Step 1, Minibatch Loss= 0.9707, Training Accuracy= 0.688\n",
      "Step 1, Minibatch Loss= 0.8783, Training Accuracy= 0.688\n",
      "Step 1, Minibatch Loss= 0.7854, Training Accuracy= 0.719\n",
      "Step 1, Minibatch Loss= 0.7048, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 1.2238, Training Accuracy= 0.531\n",
      "Step 1, Minibatch Loss= 0.7144, Training Accuracy= 0.719\n",
      "Step 1, Minibatch Loss= 0.9149, Training Accuracy= 0.562\n",
      "Step 1, Minibatch Loss= 0.6568, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 0.7005, Training Accuracy= 0.750\n",
      "Step 1, Minibatch Loss= 1.0667, Training Accuracy= 0.500\n",
      "Step 1, Minibatch Loss= 1.0661, Training Accuracy= 0.594\n",
      "Step 1, Minibatch Loss= 0.6780, Training Accuracy= 0.844\n",
      "Step 1, Minibatch Loss= 0.7227, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 2, Minibatch Loss= 0.8300, Training Accuracy= 0.781\n",
      "Step 2, Minibatch Loss= 0.7170, Training Accuracy= 0.844\n",
      "Step 2, Minibatch Loss= 0.8050, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.6314, Training Accuracy= 0.906\n",
      "Step 2, Minibatch Loss= 0.7080, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.7968, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.7483, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.6231, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.6478, Training Accuracy= 0.781\n",
      "Step 2, Minibatch Loss= 0.6687, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.6334, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 1.0231, Training Accuracy= 0.531\n",
      "Step 2, Minibatch Loss= 0.5892, Training Accuracy= 0.812\n",
      "Step 2, Minibatch Loss= 0.6775, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.8969, Training Accuracy= 0.656\n",
      "Step 2, Minibatch Loss= 0.9433, Training Accuracy= 0.688\n",
      "Step 2, Minibatch Loss= 0.8370, Training Accuracy= 0.688\n",
      "Step 2, Minibatch Loss= 0.7695, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.6914, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 1.2121, Training Accuracy= 0.531\n",
      "Step 2, Minibatch Loss= 0.6931, Training Accuracy= 0.719\n",
      "Step 2, Minibatch Loss= 0.9223, Training Accuracy= 0.562\n",
      "Step 2, Minibatch Loss= 0.6429, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 0.6811, Training Accuracy= 0.750\n",
      "Step 2, Minibatch Loss= 1.0868, Training Accuracy= 0.500\n",
      "Step 2, Minibatch Loss= 1.0536, Training Accuracy= 0.594\n",
      "Step 2, Minibatch Loss= 0.6175, Training Accuracy= 0.844\n",
      "Step 2, Minibatch Loss= 0.6899, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 3, Minibatch Loss= 0.7642, Training Accuracy= 0.781\n",
      "Step 3, Minibatch Loss= 0.6389, Training Accuracy= 0.844\n",
      "Step 3, Minibatch Loss= 0.7601, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.5648, Training Accuracy= 0.906\n",
      "Step 3, Minibatch Loss= 0.6581, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.7838, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.7394, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.5901, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.6231, Training Accuracy= 0.781\n",
      "Step 3, Minibatch Loss= 0.6516, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.6001, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 1.0184, Training Accuracy= 0.531\n",
      "Step 3, Minibatch Loss= 0.5695, Training Accuracy= 0.812\n",
      "Step 3, Minibatch Loss= 0.6535, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.8713, Training Accuracy= 0.656\n",
      "Step 3, Minibatch Loss= 0.9103, Training Accuracy= 0.688\n",
      "Step 3, Minibatch Loss= 0.8113, Training Accuracy= 0.688\n",
      "Step 3, Minibatch Loss= 0.7514, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.6745, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 1.1750, Training Accuracy= 0.531\n",
      "Step 3, Minibatch Loss= 0.6716, Training Accuracy= 0.719\n",
      "Step 3, Minibatch Loss= 0.8893, Training Accuracy= 0.562\n",
      "Step 3, Minibatch Loss= 0.6255, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 0.6671, Training Accuracy= 0.750\n",
      "Step 3, Minibatch Loss= 1.0364, Training Accuracy= 0.500\n",
      "Step 3, Minibatch Loss= 1.0416, Training Accuracy= 0.594\n",
      "Step 3, Minibatch Loss= 0.6120, Training Accuracy= 0.844\n",
      "Step 3, Minibatch Loss= 0.6748, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 4, Minibatch Loss= 0.7143, Training Accuracy= 0.781\n",
      "Step 4, Minibatch Loss= 0.6008, Training Accuracy= 0.844\n",
      "Step 4, Minibatch Loss= 0.7323, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.5304, Training Accuracy= 0.906\n",
      "Step 4, Minibatch Loss= 0.6234, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.7537, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.7140, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.5523, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.5938, Training Accuracy= 0.781\n",
      "Step 4, Minibatch Loss= 0.6190, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.5633, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 1.0166, Training Accuracy= 0.531\n",
      "Step 4, Minibatch Loss= 0.5467, Training Accuracy= 0.812\n",
      "Step 4, Minibatch Loss= 0.6250, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.8480, Training Accuracy= 0.656\n",
      "Step 4, Minibatch Loss= 0.8927, Training Accuracy= 0.688\n",
      "Step 4, Minibatch Loss= 0.7834, Training Accuracy= 0.688\n",
      "Step 4, Minibatch Loss= 0.7353, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.6530, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 1.1597, Training Accuracy= 0.531\n",
      "Step 4, Minibatch Loss= 0.6334, Training Accuracy= 0.719\n",
      "Step 4, Minibatch Loss= 0.8376, Training Accuracy= 0.562\n",
      "Step 4, Minibatch Loss= 0.5988, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.6416, Training Accuracy= 0.750\n",
      "Step 4, Minibatch Loss= 0.9692, Training Accuracy= 0.500\n",
      "Step 4, Minibatch Loss= 1.0299, Training Accuracy= 0.594\n",
      "Step 4, Minibatch Loss= 0.6155, Training Accuracy= 0.844\n",
      "Step 4, Minibatch Loss= 0.6530, Training Accuracy= 0.750\n",
      "Optimization Finished!\n",
      "Step 5, Minibatch Loss= 0.6556, Training Accuracy= 0.781\n",
      "Step 5, Minibatch Loss= 0.5614, Training Accuracy= 0.844\n",
      "Step 5, Minibatch Loss= 0.6983, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.4897, Training Accuracy= 0.906\n",
      "Step 5, Minibatch Loss= 0.5788, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.7120, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.6855, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.5069, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.5606, Training Accuracy= 0.781\n",
      "Step 5, Minibatch Loss= 0.5770, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.5223, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 1.0110, Training Accuracy= 0.531\n",
      "Step 5, Minibatch Loss= 0.5152, Training Accuracy= 0.812\n",
      "Step 5, Minibatch Loss= 0.5943, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.8093, Training Accuracy= 0.656\n",
      "Step 5, Minibatch Loss= 0.8607, Training Accuracy= 0.688\n",
      "Step 5, Minibatch Loss= 0.7377, Training Accuracy= 0.688\n",
      "Step 5, Minibatch Loss= 0.6943, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.6209, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 1.1100, Training Accuracy= 0.531\n",
      "Step 5, Minibatch Loss= 0.5594, Training Accuracy= 0.719\n",
      "Step 5, Minibatch Loss= 0.7394, Training Accuracy= 0.562\n",
      "Step 5, Minibatch Loss= 0.5594, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.5993, Training Accuracy= 0.750\n",
      "Step 5, Minibatch Loss= 0.8700, Training Accuracy= 0.500\n",
      "Step 5, Minibatch Loss= 1.0195, Training Accuracy= 0.625\n",
      "Step 5, Minibatch Loss= 0.6298, Training Accuracy= 0.844\n",
      "Step 5, Minibatch Loss= 0.6148, Training Accuracy= 0.781\n",
      "Optimization Finished!\n",
      "Step 6, Minibatch Loss= 0.5748, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.4985, Training Accuracy= 0.844\n",
      "Step 6, Minibatch Loss= 0.6372, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.4178, Training Accuracy= 0.906\n",
      "Step 6, Minibatch Loss= 0.5045, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.6592, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.6459, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.4300, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.5135, Training Accuracy= 0.781\n",
      "Step 6, Minibatch Loss= 0.5120, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.4735, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 1.0045, Training Accuracy= 0.531\n",
      "Step 6, Minibatch Loss= 0.4775, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 0.5550, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.7488, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.8088, Training Accuracy= 0.688\n",
      "Step 6, Minibatch Loss= 0.6624, Training Accuracy= 0.656\n",
      "Step 6, Minibatch Loss= 0.6301, Training Accuracy= 0.719\n",
      "Step 6, Minibatch Loss= 0.5855, Training Accuracy= 0.812\n",
      "Step 6, Minibatch Loss= 1.0375, Training Accuracy= 0.531\n",
      "Step 6, Minibatch Loss= 0.4821, Training Accuracy= 0.844\n",
      "Step 6, Minibatch Loss= 0.6074, Training Accuracy= 0.625\n",
      "Step 6, Minibatch Loss= 0.5149, Training Accuracy= 0.781\n",
      "Step 6, Minibatch Loss= 0.5319, Training Accuracy= 0.750\n",
      "Step 6, Minibatch Loss= 0.7700, Training Accuracy= 0.594\n",
      "Step 6, Minibatch Loss= 0.9711, Training Accuracy= 0.594\n",
      "Step 6, Minibatch Loss= 0.5814, Training Accuracy= 0.844\n",
      "Step 6, Minibatch Loss= 0.5312, Training Accuracy= 0.812\n",
      "Optimization Finished!\n",
      "Step 7, Minibatch Loss= 0.4879, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.4130, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.5578, Training Accuracy= 0.750\n",
      "Step 7, Minibatch Loss= 0.3332, Training Accuracy= 0.938\n",
      "Step 7, Minibatch Loss= 0.4384, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.6088, Training Accuracy= 0.688\n",
      "Step 7, Minibatch Loss= 0.6229, Training Accuracy= 0.688\n",
      "Step 7, Minibatch Loss= 0.3671, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.4700, Training Accuracy= 0.781\n",
      "Step 7, Minibatch Loss= 0.4394, Training Accuracy= 0.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7, Minibatch Loss= 0.4250, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.9267, Training Accuracy= 0.562\n",
      "Step 7, Minibatch Loss= 0.4316, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.4935, Training Accuracy= 0.781\n",
      "Step 7, Minibatch Loss= 0.6564, Training Accuracy= 0.719\n",
      "Step 7, Minibatch Loss= 0.7296, Training Accuracy= 0.656\n",
      "Step 7, Minibatch Loss= 0.5939, Training Accuracy= 0.688\n",
      "Step 7, Minibatch Loss= 0.5586, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.5529, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.9820, Training Accuracy= 0.625\n",
      "Step 7, Minibatch Loss= 0.4038, Training Accuracy= 0.906\n",
      "Step 7, Minibatch Loss= 0.5027, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.4422, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.4253, Training Accuracy= 0.812\n",
      "Step 7, Minibatch Loss= 0.6901, Training Accuracy= 0.625\n",
      "Step 7, Minibatch Loss= 0.9002, Training Accuracy= 0.625\n",
      "Step 7, Minibatch Loss= 0.5184, Training Accuracy= 0.844\n",
      "Step 7, Minibatch Loss= 0.4680, Training Accuracy= 0.844\n",
      "Optimization Finished!\n",
      "Step 8, Minibatch Loss= 0.4137, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.3688, Training Accuracy= 0.906\n",
      "Step 8, Minibatch Loss= 0.5096, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.2894, Training Accuracy= 0.906\n",
      "Step 8, Minibatch Loss= 0.3929, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.5222, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 0.5624, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.3194, Training Accuracy= 0.875\n",
      "Step 8, Minibatch Loss= 0.4237, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.3525, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.3808, Training Accuracy= 0.906\n",
      "Step 8, Minibatch Loss= 0.8467, Training Accuracy= 0.594\n",
      "Step 8, Minibatch Loss= 0.3809, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.4338, Training Accuracy= 0.781\n",
      "Step 8, Minibatch Loss= 0.5732, Training Accuracy= 0.781\n",
      "Step 8, Minibatch Loss= 0.6797, Training Accuracy= 0.688\n",
      "Step 8, Minibatch Loss= 0.5263, Training Accuracy= 0.750\n",
      "Step 8, Minibatch Loss= 0.4931, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.4949, Training Accuracy= 0.812\n",
      "Step 8, Minibatch Loss= 0.9199, Training Accuracy= 0.562\n",
      "Step 8, Minibatch Loss= 0.3146, Training Accuracy= 0.938\n",
      "Step 8, Minibatch Loss= 0.4085, Training Accuracy= 0.906\n",
      "Step 8, Minibatch Loss= 0.3619, Training Accuracy= 0.969\n",
      "Step 8, Minibatch Loss= 0.3480, Training Accuracy= 0.844\n",
      "Step 8, Minibatch Loss= 0.5989, Training Accuracy= 0.781\n",
      "Step 8, Minibatch Loss= 0.8064, Training Accuracy= 0.688\n",
      "Step 8, Minibatch Loss= 0.4479, Training Accuracy= 0.875\n",
      "Step 8, Minibatch Loss= 0.4240, Training Accuracy= 0.812\n",
      "Optimization Finished!\n",
      "Step 9, Minibatch Loss= 0.3521, Training Accuracy= 0.938\n",
      "Step 9, Minibatch Loss= 0.3324, Training Accuracy= 0.938\n",
      "Step 9, Minibatch Loss= 0.4578, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.2446, Training Accuracy= 1.000\n",
      "Step 9, Minibatch Loss= 0.3462, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.4559, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.5207, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.2839, Training Accuracy= 0.906\n",
      "Step 9, Minibatch Loss= 0.3580, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.2743, Training Accuracy= 0.938\n",
      "Step 9, Minibatch Loss= 0.3427, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.7295, Training Accuracy= 0.688\n",
      "Step 9, Minibatch Loss= 0.3302, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.3726, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.4943, Training Accuracy= 0.844\n",
      "Step 9, Minibatch Loss= 0.6179, Training Accuracy= 0.750\n",
      "Step 9, Minibatch Loss= 0.4570, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.4366, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.4327, Training Accuracy= 0.812\n",
      "Step 9, Minibatch Loss= 0.8536, Training Accuracy= 0.594\n",
      "Step 9, Minibatch Loss= 0.2456, Training Accuracy= 0.938\n",
      "Step 9, Minibatch Loss= 0.3423, Training Accuracy= 0.938\n",
      "Step 9, Minibatch Loss= 0.2848, Training Accuracy= 0.969\n",
      "Step 9, Minibatch Loss= 0.2845, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.5325, Training Accuracy= 0.781\n",
      "Step 9, Minibatch Loss= 0.7135, Training Accuracy= 0.719\n",
      "Step 9, Minibatch Loss= 0.3577, Training Accuracy= 0.875\n",
      "Step 9, Minibatch Loss= 0.3776, Training Accuracy= 0.875\n",
      "Optimization Finished!\n",
      "Step 10, Minibatch Loss= 0.2978, Training Accuracy= 0.969\n",
      "Step 10, Minibatch Loss= 0.3062, Training Accuracy= 0.938\n",
      "Step 10, Minibatch Loss= 0.4047, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.2184, Training Accuracy= 1.000\n",
      "Step 10, Minibatch Loss= 0.3123, Training Accuracy= 0.906\n",
      "Step 10, Minibatch Loss= 0.3621, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.4602, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.2483, Training Accuracy= 0.938\n",
      "Step 10, Minibatch Loss= 0.2960, Training Accuracy= 0.906\n",
      "Step 10, Minibatch Loss= 0.2221, Training Accuracy= 0.938\n",
      "Step 10, Minibatch Loss= 0.3065, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.6298, Training Accuracy= 0.688\n",
      "Step 10, Minibatch Loss= 0.2874, Training Accuracy= 0.844\n",
      "Step 10, Minibatch Loss= 0.3164, Training Accuracy= 0.906\n",
      "Step 10, Minibatch Loss= 0.4225, Training Accuracy= 0.844\n",
      "Step 10, Minibatch Loss= 0.5563, Training Accuracy= 0.781\n",
      "Step 10, Minibatch Loss= 0.3807, Training Accuracy= 0.844\n",
      "Step 10, Minibatch Loss= 0.3780, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.3775, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.7842, Training Accuracy= 0.656\n",
      "Step 10, Minibatch Loss= 0.1973, Training Accuracy= 0.969\n",
      "Step 10, Minibatch Loss= 0.2715, Training Accuracy= 0.938\n",
      "Step 10, Minibatch Loss= 0.2172, Training Accuracy= 0.969\n",
      "Step 10, Minibatch Loss= 0.2395, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.4606, Training Accuracy= 0.812\n",
      "Step 10, Minibatch Loss= 0.6097, Training Accuracy= 0.750\n",
      "Step 10, Minibatch Loss= 0.2733, Training Accuracy= 0.875\n",
      "Step 10, Minibatch Loss= 0.3197, Training Accuracy= 0.906\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#print(len(train_Y))\n",
    "for step in range(1, training_steps+1):\n",
    "    i = 0\n",
    "    while i < len(train_sents):\n",
    "        x_batch, y_batch, i = batch_maker(train_sents, train_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "        #print(i, x_batch.shape, y_batch.shape)\n",
    "        #x_lookup = tf.nn.embedding_lookup(word_embs, x_batch)\n",
    "        #sess.run(x_lookup)\n",
    "        sess.run(train_op, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "        #print('HUY')\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "            \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.35374149659863946, 1: 0, 2: 0.6964856230031948} 0.35007570653394476\n"
     ]
    }
   ],
   "source": [
    "all_answers = np.array([])\n",
    "all_true_labels = np.array([])\n",
    "i = 0\n",
    "\n",
    "while i < len(test_sents):\n",
    "    x_batch, y_batch, i = batch_maker(test_sents, test_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "    \n",
    "    a, t = sess.run([answers, true_labels], feed_dict={X: x_batch, Y: y_batch})\n",
    "    #print(a,t)\n",
    "    all_answers = np.append(all_answers, np.asarray(a)) #no need to flatten\n",
    "    all_true_labels = np.append(all_true_labels, np.asarray(t))\n",
    "    \n",
    "    \n",
    "TP, FP, FN = {0:0, 1:0, 2:0}, {0:0, 1:0, 2:0}, {0:0, 1:0, 2:0}\n",
    "for l in range(3):\n",
    "    for counter in range(len(all_answers)):\n",
    "        if all_answers[counter] == all_true_labels[counter] and all_answers[counter] == l: #answer matches label\n",
    "            TP[l] += 1\n",
    "        if all_answers[counter] != all_true_labels[counter] and all_answers[counter] == l:\n",
    "            FP[l] += 1\n",
    "        if all_answers[counter] != all_true_labels[counter] and all_answers[counter] != l:\n",
    "            FN[l] += 1\n",
    "            \n",
    "Prec, Rec = {}, {}\n",
    "for l in range(3):\n",
    "    try:\n",
    "        Prec[l] = TP[l]/float((TP[l]+FP[l]))\n",
    "    except Exception:\n",
    "        Prec[l] = 0\n",
    "    try:\n",
    "        Rec[l] = TP[l]/float((TP[l]+FN[l]))\n",
    "    except Exception:\n",
    "        Rec[l] = 0\n",
    "F1 = {}\n",
    "for l in range(3):\n",
    "    try:\n",
    "        F1[l] = 2*Prec[l]*Rec[l]/(Prec[l]+Rec[l])\n",
    "    except Exception:\n",
    "        F1[l] = 0\n",
    "    \n",
    "    \n",
    "MF1 = sum(_ for _ in F1.values())/3\n",
    "print(F1, MF1)\n",
    "#print(TP, FP, FN, Prec, Rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5869565217391305\n"
     ]
    }
   ],
   "source": [
    "preds = np.array([])\n",
    "i = 0\n",
    "while i < len(test_sents):\n",
    "    x_batch, y_batch, i = batch_maker(test_sents, test_Y, min_samples = 32, sample_size = timesteps, pos = i)\n",
    "        #print(i, x_batch.shape, y_batch.shape)\n",
    "        #x_lookup = tf.nn.embedding_lookup(word_embs, x_batch)\n",
    "        #sess.run(x_lookup)\n",
    "        #sess.run(train_op, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "        #print('HUY')\n",
    "             \n",
    "    pred = sess.run(correct_pred, feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "    preds = np.append(preds, np.asarray(pred))\n",
    "print(np.mean(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
