{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 50\n",
    "embedding_dims = 50\n",
    "n_filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "learning_rate = 0.003\n",
    "training_steps = 2\n",
    "width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [3 4 5 6]\n",
      " [0 0 7 8]]\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "a = sequence.pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])\n",
    "print(a)\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"int32\", [batch_size, maxlen])\n",
    "Y = tf.placeholder(\"float32\", [batch_size,]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "word_embs  = tf.Variable(xavier_init([max_features, embedding_dims]))\n",
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "filters = tf.Variable(xavier_init([width, embedding_dims, n_filters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    #print(tf.shape(x))\n",
    "    \n",
    "    x = tf.nn.embedding_lookup(word_embs, x) #embedding - returns batch x len x emb ????\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.dropout(inputs = x, rate = 0.2)\n",
    "    #print(x.dtype)\n",
    "    x = tf.nn.conv1d(value=x, filters = filters, stride = 1,padding = 'VALID')\n",
    "    x = tf.nn.relu(features = x)\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.max_pooling1d(inputs = x, pool_size = maxlen - 2, strides = 1, padding = 'VALID') #pool_size = tf.shape(x)[1]\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.squeeze(x, [1]) #because of one fictional dim from global max pooling\n",
    "    x = tf.layers.dense(inputs = x, units = 250, activation = 'relu') #has his variables inside?\n",
    "    x = tf.layers.dropout(inputs = x, rate = 0.2)\n",
    "    #print(tf.shape(x))\n",
    "    x = tf.layers.dense(inputs = x, units = 1)\n",
    "    #x = tf.nn.sigmoid(x)\n",
    "    #print(tf.shape(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.squeeze(neural_net(X), [1])\n",
    "\n",
    "#this WAS in LSTM. needed? \n",
    "prediction_sm = tf.nn.sigmoid(logits) # THIS IS THE ANSWER ALREADY\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y)) #there is softmax applied to the last output\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#print('\\n',tf.shape(prediction), tf.shape(tf.argmax(prediction, 1)), tf.shape(tf.argmax(Y, 1)))\n",
    "correct_pred = tf.equal(prediction_sm, Y)#tf.argmax(logits, 1),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#print(len(train_Y))\n",
    "for step in range(1, training_steps+1):\n",
    "    i = 0\n",
    "    while i*batch_size < len(x_train):\n",
    "        \n",
    "        x_batch, y_batch = x_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        i += 1\n",
    "        #print(i, x_batch.shape, y_batch.shape)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: x_batch, Y: y_batch}) \n",
    "        \n",
    "        #oss, acc = sess.run([loss_op, accuracy], feed_dict={X: x_batch, Y: y_batch}) #x_lookup\n",
    "        #    \n",
    "        #    print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "        #          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "        #          \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8801612107254482, 1: 0.8865441520012459} 0.883352681363347\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([])\n",
    "\n",
    "i = 0\n",
    "while i*batch_size < len(x_test):\n",
    "    x_batch, y_batch = x_test[i*batch_size:(i+1)*batch_size], y_test[i*batch_size:(i+1)*batch_size]\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "    a = sess.run(prediction_sm, feed_dict={X: x_batch, Y: y_batch})\n",
    "    #print(a,t)\n",
    "    prediction = np.append(prediction, np.asarray(a)) #no need to flatten\n",
    "    \n",
    "prediction = [int(round(t)) for t in prediction]\n",
    "\n",
    "TP, FP, FN = {0:0, 1:0}, {0:0, 1:0}, {0:0, 1:0}\n",
    "for l in range(2):\n",
    "    for counter in range(len(prediction)):\n",
    "        if prediction[counter] == y_test[counter] and prediction[counter] == l: #answer matches label\n",
    "            TP[l] += 1\n",
    "        if prediction[counter] != y_test[counter] and prediction[counter] == l:\n",
    "            FP[l] += 1\n",
    "        if prediction[counter] != y_test[counter] and prediction[counter] != l:\n",
    "            FN[l] += 1\n",
    "            \n",
    "Prec, Rec = {}, {}\n",
    "for l in range(2):\n",
    "    try:\n",
    "        Prec[l] = TP[l]/float((TP[l]+FP[l]))\n",
    "    except Exception:\n",
    "        Prec[l] = 0\n",
    "    try:\n",
    "        Rec[l] = TP[l]/float((TP[l]+FN[l]))\n",
    "    except Exception:\n",
    "        Rec[l] = 0\n",
    "F1 = {}\n",
    "for l in range(2):\n",
    "    try:\n",
    "        F1[l] = 2*Prec[l]*Rec[l]/(Prec[l]+Rec[l])\n",
    "    except Exception:\n",
    "        F1[l] = 0\n",
    "    \n",
    "    \n",
    "MF1 = sum(_ for _ in F1.values())/2\n",
    "print(F1, MF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01761236 0.986368   0.97832263 ... 0.01595182 0.12709025 0.33885211]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "h = 0.9999\n",
    "\n",
    "print(int(round(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
